{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VRNN_Train_MNIST.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0H_0KojHoNUc"},"source":["Mount Drive"]},{"cell_type":"code","metadata":{"id":"hUhDBpltG2qS","outputId":"1ec9682e-702d-4660-bc46-75cd2c2ea06e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xQR3UP3LAtAq","outputId":"cc8ddc0e-8a61-4866-f923-00a24ae465e7","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd '/content/gdrive/My Drive/VIP_Project/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/VIP_Project\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nAWR6LKHpXYH"},"source":["Define Model"]},{"cell_type":"code","metadata":{"id":"kGRt5xyJpSmw","outputId":"032d362f-0dbe-4856-e6a4-35ebcd7de377","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from pylab import rcParams"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZSh6u90zSIyc","outputId":"cf3f9a81-4672-4cef-a07f-49f2bfb08a24","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["num_epoch = 100\n","batch_size = 50\n","\n","in_timesteps  = range(0,19)\n","out_timesteps = range(1,20)\n","\n","# prepare data\n","data = np.load(\"moving-mnist-2-tr-images.npy\" )\n","\n","# training set\n","x_tr = data[0:10000, in_timesteps,  :, :]\n","y_tr = data[0:10000, out_timesteps, :, :]\n","x_tr = x_tr.reshape(x_tr.shape[0], len(in_timesteps),  64, 64, 1)\n","y_tr = y_tr.reshape(y_tr.shape[0], len(out_timesteps), 64, 64, 1)\n","tr_set = data[0:10000, :, :, :]\n","\n","# validation set\n","x_te = data[10000:, in_timesteps,  :, :]\n","y_te = data[10000:, out_timesteps, :, :]\n","x_te = x_te.reshape(x_te.shape[0], len(in_timesteps),  64, 64, 1)\n","y_te = y_te.reshape(y_te.shape[0], len(out_timesteps), 64, 64, 1)\n","te_set = data[10000:, :, :, :]\n","\n","print (np.shape(x_tr), np.shape(y_tr), np.shape(x_te), np.shape(y_te))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(10000, 19, 64, 64, 1) (10000, 19, 64, 64, 1) (1000, 19, 64, 64, 1) (1000, 19, 64, 64, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CX6voHEvpbF0","outputId":"67a919a0-fd56-4401-99e1-5f36e8ab2899","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["lstm_units = 1024\n","feature_vector = 1024\n","latent_dim = 128\n","\n","# placeholders to hold each frame\n","x_ = tf.placeholder(\"float32\", shape= (None, len(in_timesteps),  64, 64, 1))\n","y_ = tf.placeholder(\"float32\", shape= (None, len(out_timesteps), 64, 64, 1))\n","\n","# encoder\n","encoder_conv1_w = tf.get_variable(\"encoder_conv1_w\", shape=[7, 7, 1, 16])\n","encoder_conv2_w = tf.get_variable(\"encoder_conv2_w\", shape=[5, 5, 16, 32])\n","encoder_conv3_w = tf.get_variable(\"encoder_conv3_w\", shape=[5, 5, 32, 48])\n","encoder_conv4_w = tf.get_variable(\"encoder_conv4_w\", shape=[3, 3, 48, 64])\n","\n","encoder_conv1_b = tf.get_variable(\"encoder_conv1_b\", shape=[16])\n","encoder_conv2_b = tf.get_variable(\"encoder_conv2_b\", shape=[32])\n","encoder_conv3_b = tf.get_variable(\"encoder_conv3_b\", shape=[48])\n","encoder_conv4_b = tf.get_variable(\"encoder_conv4_b\", shape=[64])\n","\n","def encoder(x):\n","    out = tf.nn.conv2d(input=x,   filter=encoder_conv1_w, strides=[1, 2, 2, 1], padding='SAME') + encoder_conv1_b\n","    out = tf.nn.relu(out)\n","    out = tf.nn.conv2d(input=out, filter=encoder_conv2_w, strides=[1, 2, 2, 1], padding='SAME') + encoder_conv2_b\n","    out = tf.nn.relu(out)\n","    out = tf.nn.conv2d(input=out, filter=encoder_conv3_w, strides=[1, 2, 2, 1], padding='SAME') + encoder_conv3_b\n","    out = tf.nn.relu(out)\n","    out = tf.nn.conv2d(input=out, filter=encoder_conv4_w, strides=[1, 2, 2, 1], padding='SAME') + encoder_conv4_b\n","    out = tf.nn.relu(out)\n","    out = tf.reshape(out, shape=[-1, 4*4*64])\n","    return out\n","\n","# decoder\n","decoder_conv1_w = tf.get_variable(\"decoder_conv1_w\", shape=[3, 3, 48, 64])\n","decoder_conv2_w = tf.get_variable(\"decoder_conv2_w\", shape=[5, 5, 32, 48])\n","decoder_conv3_w = tf.get_variable(\"decoder_conv3_w\", shape=[5, 5, 16, 32])\n","decoder_conv4_w = tf.get_variable(\"decoder_conv4_w\", shape=[7, 7, 1, 16])\n","\n","decoder_conv1_b = tf.get_variable(\"decoder_conv1_b\", shape=[48])\n","decoder_conv2_b = tf.get_variable(\"decoder_conv2_b\", shape=[32])\n","decoder_conv3_b = tf.get_variable(\"decoder_conv3_b\", shape=[16])\n","decoder_conv4_b = tf.get_variable(\"decoder_conv4_b\", shape=[1])\n","\n","def decoder(x):\n","    out = tf.reshape(x, shape=[-1, 4, 4, 64])\n","    out = tf.nn.conv2d_transpose(out, filter=decoder_conv1_w, strides=[1, 2, 2, 1], output_shape=[batch_size, 8, 8, 48], padding='SAME') + decoder_conv1_b\n","    out = tf.nn.relu(out)\n","    out = tf.nn.conv2d_transpose(out, filter=decoder_conv2_w, strides=[1, 2, 2, 1], output_shape=[batch_size, 16, 16, 32], padding='SAME') + decoder_conv2_b\n","    out = tf.nn.relu(out)\n","    out = tf.nn.conv2d_transpose(out, filter=decoder_conv3_w, strides=[1, 2, 2, 1], output_shape=[batch_size, 32, 32, 16], padding='SAME') + decoder_conv3_b\n","    out = tf.nn.relu(out)\n","    out = tf.nn.conv2d_transpose(out, filter=decoder_conv4_w, strides=[1, 2, 2, 1], output_shape=[batch_size, 64, 64, 1], padding='SAME') + decoder_conv4_b\n","    out = tf.nn.sigmoid(out)\n","    return out\n","\n","# f_posterior\n","f_posterior_fc1_w = tf.get_variable(\"phi_enc_fc1_w\", shape=[feature_vector+lstm_units, latent_dim])\n","f_posterior_fc2_w = tf.get_variable(\"phi_enc_fc2_w\", shape=[latent_dim, latent_dim])\n","\n","f_posterior_fc1_b = tf.get_variable(\"phi_enc_fc1_b\", shape=[latent_dim])\n","f_posterior_fc2_b = tf.get_variable(\"phi_enc_fc2_b\", shape=[latent_dim])\n","\n","f_posterior_mu_w = tf.get_variable(\"phi_enc_mu_w\", shape=[latent_dim, latent_dim])\n","f_posterior_mu_b = tf.get_variable(\"phi_enc_mu_b\", shape=[latent_dim])\n","\n","f_posterior_sigma_w = tf.get_variable(\"phi_enc_sigma_w\", shape=[latent_dim, latent_dim])\n","f_posterior_sigma_b = tf.get_variable(\"phi_enc_sigma_b\", shape=[latent_dim])\n","\n","def f_posterior(out):\n","    out = tf.matmul(out, f_posterior_fc1_w) + f_posterior_fc1_b    \n","    out = tf.nn.relu(out)\n","    out = tf.matmul(out, f_posterior_fc2_w) + f_posterior_fc2_b\n","    out = tf.nn.relu(out)\n","    \n","    out_mu  = tf.matmul(out, f_posterior_mu_w)  + f_posterior_mu_b\n","    out_std = tf.nn.softplus(tf.matmul(out, f_posterior_sigma_w) + f_posterior_sigma_b)\n","    \n","    return out_mu, out_std\n","\n","# f_decoder\n","f_decoder_fc1_w = tf.get_variable(\"phi_dec_fc1_w\", shape=[latent_dim+lstm_units, feature_vector])\n","f_decoder_fc2_w = tf.get_variable(\"phi_dec_fc2_w\", shape=[feature_vector, feature_vector])\n","\n","f_decoder_fc1_b = tf.get_variable(\"phi_dec_fc1_b\", shape=[feature_vector])\n","f_decoder_fc2_b = tf.get_variable(\"phi_dec_fc2_b\", shape=[feature_vector])\n","\n","def f_decoder(out):\n","    out = tf.matmul(out, f_decoder_fc1_w) + f_decoder_fc1_b\n","    out = tf.nn.relu(out)\n","    out = tf.matmul(out, f_decoder_fc2_w) + f_decoder_fc2_b\n","    out = tf.nn.relu(out)\n","        \n","    return out\n","\n","# f_z\n","f_z_fc1_w = tf.get_variable(\"phi_z_fc1_w\", shape=[latent_dim, latent_dim])\n","f_z_fc2_w = tf.get_variable(\"phi_z_fc2_w\", shape=[latent_dim, latent_dim])\n","\n","f_z_fc1_b = tf.get_variable(\"phi_z_fc1_b\", shape=[latent_dim])\n","f_z_fc2_b = tf.get_variable(\"phi_z_fc2_b\", shape=[latent_dim])\n","\n","def f_z(out):\n","    out = tf.matmul(out, f_z_fc1_w) + f_z_fc1_b\n","    out = tf.nn.relu(out)\n","    out = tf.matmul(out, f_z_fc2_w) + f_z_fc2_b\n","    out = tf.nn.relu(out)\n","    return out\n","\n","# f_prior\n","f_prior_fc1_w = tf.get_variable(\"phi_prior_fc1_w\", shape=[lstm_units, latent_dim])\n","f_prior_fc2_w = tf.get_variable(\"phi_prior_fc2_w\", shape=[latent_dim, latent_dim])\n","\n","f_prior_fc1_b = tf.get_variable(\"phi_prior_fc1_b\", shape=[latent_dim])\n","f_prior_fc2_b = tf.get_variable(\"phi_prior_fc2_b\", shape=[latent_dim])\n","\n","f_prior_mu_w = tf.get_variable(\"phi_prior_mu_w\", shape=[latent_dim, latent_dim])\n","f_prior_std_w = tf.get_variable(\"phi_prior_std_w\", shape=[latent_dim, latent_dim])\n","\n","f_prior_mu_b = tf.get_variable(\"phi_prior_mu_b\", shape=[latent_dim])\n","f_prior_std_b = tf.get_variable(\"phi_prior_std_b\", shape=[latent_dim])\n","\n","def f_prior(out):\n","    out = tf.matmul(out, f_prior_fc1_w) + f_prior_fc1_b\n","    out = tf.nn.relu(out)\n","    out = tf.matmul(out, f_prior_fc2_w) + f_prior_fc2_b\n","    out = tf.nn.relu(out)\n","    \n","    out_mu  = tf.matmul(out, f_prior_mu_w)  + f_prior_mu_b\n","    out_std = tf.nn.softplus(tf.matmul(out, f_prior_std_w) + f_prior_std_b)\n","    \n","    return out_mu, out_std\n","\n","def tf_kl_gaussgauss(mu_1, sigma_1, mu_2, sigma_2):\n","    return tf.reduce_sum(tf.log(sigma_2) - tf.log(sigma_1) + (sigma_1**2 + (mu_1 - mu_2)**2) / (2*((sigma_2)**2)) - 0.5, axis=1)\n","\n","def tf_kl_gaussgauss2(mu_1, sigma_1, mu_2, sigma_2):\n","    return tf.reduce_mean(tf.log(sigma_2) - tf.log(sigma_1) + (sigma_1**2 + (mu_1 - mu_2)**2) / (2*((sigma_2)**2)) - 0.5, axis=0)\n","\n","def cross_entropy(y_prediction, y):\n","    prediction_loss = y * tf.log(1e-10 + y_prediction) + (1 - y) * tf.log(1e-10 + 1 - y_prediction)\n","    return -tf.reduce_sum(prediction_loss, axis=[1, 2, 3])   \n","\n","def batch_data(source, target, batch_size):\n","\n","   # Shuffle data\n","    shuffle_indices = np.random.permutation(np.arange(len(target)))\n","    source = source[shuffle_indices]\n","    target = target[shuffle_indices]\n","\n","    for batch_i in range(0, len(source)//batch_size):\n","        start_i = batch_i * batch_size\n","        source_batch = source[start_i:start_i + batch_size]\n","        target_batch = target[start_i:start_i + batch_size]\n","\n","        yield np.array(source_batch), np.array(target_batch)\n","\n","# lstm\n","lstm  = tf.nn.rnn_cell.LSTMCell(num_units = lstm_units, state_is_tuple=True)\n","lstm_state = lstm.zero_state(batch_size, tf.float32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-7-ebf45fec3515>:157: LSTMCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z_HdlsWJplKJ","outputId":"90ce364b-5e92-4bd0-9c0c-891affe74f91","colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["loss_list = [None]*19\n","reconstruction_loss_list = [None]*19\n","kl_divergence_list = [None]*19\n","kl_divergence_list2 = [None]*19\n","    \n","for i in range(0,len(in_timesteps)):\n","        \n","    # encode image\n","    encoder_out_out = encoder(tf.divide(x=x_[:,i,:,:,:],y=255.0))\n","\n","    # compute prior\n","    f_prior_out_mu, f_prior_out_sigma = f_prior(lstm_state[1])\n","\n","    # compute posterior\n","    f_posterior_out_mu, f_posterior_out_sigma = f_posterior(tf.concat(values=(lstm_state[1], encoder(tf.divide(x=y_[:,i,:,:,:],y=255.0))), axis=1))\n","   \n","    # sample from posterior \n","    z = f_posterior_out_mu + f_posterior_out_sigma * tf.random_normal(shape=[latent_dim], mean=0.0, stddev=1.0)        \n","    f_z_out = f_z(z)\n","\n","    # decode [lstm, latent information]\n","    f_decoder_out = f_decoder(tf.concat(values=(lstm_state[1], f_z_out), axis=1)) \n","    y_hat = decoder(f_decoder_out)\n","\n","    # lstm state transition\n","    lstm_out, lstm_state = lstm(inputs = tf.concat(values=(encoder_out_out, f_z_out), axis=1), state = lstm_state)\n","        \n","    # track divergence of current timestep\n","    kl_divergence_list[i] = tf_kl_gaussgauss(f_posterior_out_mu, f_posterior_out_sigma, f_prior_out_mu, f_prior_out_sigma)    \n","    tf.summary.scalar(\"kl_divergence_loss_\" + str(i), tf.reduce_mean(kl_divergence_list[i]))\n","                \n","    # track reconstruction loss of current timestep\n","    reconstruction_loss_list[i] = cross_entropy(y_hat,tf.divide(x=y_[:,i,:,:,:],y=255.0))\n","    tf.summary.scalar(\"reconstruction_loss_\" + str(i), tf.reduce_mean(reconstruction_loss_list[i]))\n","         \n","    # track total loss of current timestep\n","    loss_list[i] = tf.reduce_mean(kl_divergence_list[i] + reconstruction_loss_list[i])\n","    tf.summary.scalar(\"total_loss_\" + str(i), loss_list[i])\n","\n","# optimize loss and track its mean across the 19 timesteps\n","loss = tf.stack(loss_list)\n","optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(loss)\n","tf.summary.scalar(\"total_loss_mean\", tf.reduce_mean(loss))\n","    \n","# track mean of kl divergence across the 19 timesteps\n","kl_divergence = tf.stack(kl_divergence_list)\n","tf.summary.scalar(\"kl_divergence_loss_mean\", tf.reduce_mean(kl_divergence))\n","\n","# track mean of reconstruction loss across the 19 timesteps\n","reconstruction_loss = tf.stack(reconstruction_loss_list)\n","tf.summary.scalar(\"reconstruction_loss_mean\", tf.reduce_mean(reconstruction_loss))\n","    \n","# train and validation counters\n","train_counter = tf.Variable(0, name=\"train_counter\", trainable=False)\n","increment_train_counter = tf.assign(train_counter, train_counter+1)\n","validation_counter = tf.Variable(0, name=\"validation_counter\", trainable=False)\n","increment_validation_counter = tf.assign(validation_counter, train_counter+1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:966: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:970: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DwnwR4Tipoci"},"source":["Start Training"]},{"cell_type":"code","metadata":{"id":"DY4wSvptpn1F"},"source":["# Summaries\n","merged_summary_op = tf.summary.merge_all()\n","train_summary_writer = tf.summary.FileWriter('./train/', graph=tf.get_default_graph())\n","validation_summary_writer = tf.summary.FileWriter('./validation/',graph=tf.get_default_graph())\n","\n","# initialize all variables\n","init = tf.global_variables_initializer()\n","\n","# to save variables\n","saver = tf.train.Saver(max_to_keep=100)\n","\n","# Start a new TF session\n","sess = tf.Session()\n","\n","# Run the initializer\n","sess.run(init)\n","\n","# Train\n","for i in range(1, 101):\n","    \n","    print ('Epoch ', i)\n","    \n","    # Train\n","    for x_tr_batch, y_tr_batch in batch_data(x_tr, y_tr, batch_size=batch_size):\n","        _, step, summary = sess.run([optimizer, increment_train_counter, merged_summary_op], feed_dict={x_: x_tr_batch, y_: y_tr_batch})\n","        train_summary_writer.add_summary(summary, step) \n","            \n","    # Validate\n","    for x_te_batch, y_te_batch in batch_data(x_te, y_te, batch_size=batch_size):\n","        step, summary = sess.run([increment_validation_counter, merged_summary_op], feed_dict={x_: x_te_batch, y_: y_te_batch}) \n","        validation_summary_writer.add_summary(summary, step)\n","    \n","save_path = saver.save(sess, 'epoch', i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWBQSqvhwlSR"},"source":["Architecture"]},{"cell_type":"code","metadata":{"id":"-SQmwIrRz3G6"},"source":["|"],"execution_count":null,"outputs":[]}]}